{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "name": "Tutorial1 - Statistical Learning",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/german1728/Chimera/blob/master/Tutorial1_Statistical_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhxYC7KXsjT9"
      },
      "source": [
        "# Tutorial 1 - Machine Learning\n",
        "# Regresión Lineal con MSE\n",
        "\n",
        "**Content creators**: Pierre-Étienne Fiquet, Anqi Wu, Alex Hyafil with help from Byron Galbraith\n",
        "\n",
        "**Content reviewers**: Lina Teichmann, Saeed Salehi, Patrick Mineault,  Ella Batty, Michael Waskom\n",
        "\n",
        "**Content traduction**: Israel Chaparro\n",
        "\n",
        "Source: Neuromatch Academy\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTtUhpSJsjT-"
      },
      "source": [
        "___\n",
        "#Objetivos del Tutorial\n",
        "\n",
        "Este es el Tutorial 1 de una serie sobre cómo ajustar modelos a datos. Comenzamos con regresión lineal simple, usando optimización de mínimos cuadrados (Tutorial 1) y Estimación de máxima verosimilitud (Tutorial 2). Usaremos bootstrapping para construir intervalos de confianza alrededor de los parámetros del modelo lineal inferido (Tutorial 3). Terminaremos nuestra exploración de modelos de regresión generalizando a la regresión lineal múltiple y la regresión polinomial (Tutorial 4). Terminamos aprendiendo a elegir entre estos distintos modelos. Analizamos la compensación de sesgo-varianza (Tutorial 5) y la Validación cruzada para la selección del modelo (Tutorial 6).\n",
        "\n",
        "En este tutorial, aprenderemos cómo ajustar modelos lineales simples a los datos.\n",
        "- Aprenda a calcular el error cuadrático medio (MSE)\n",
        "- Explore cómo los parámetros del modelo (pendiente) influyen en el MSE\n",
        "- Aprenda a encontrar el parámetro de modelo óptimo mediante la optimización de mínimos cuadrados\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxgZcf1-sjT_"
      },
      "source": [
        "---\n",
        "# Configuraciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "Hx0rK5EusjUA"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLZpP8CXsjUF"
      },
      "source": [
        "#@title Configuraciones de Figuras\n",
        "import ipywidgets as widgets       # interactive display\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gXmYsbasjUK"
      },
      "source": [
        "#@title Funciones de ayuda\n",
        "\n",
        "def plot_observed_vs_predicted(x, y, y_hat, theta_hat):\n",
        "  \"\"\" Plot observed vs predicted data\n",
        "\n",
        "  Args:\n",
        "      x (ndarray): observed x values\n",
        "  y (ndarray): observed y values\n",
        "  y_hat (ndarray): predicted y values\n",
        "\n",
        "  \"\"\"\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.scatter(x, y, label='Observed')  # our data scatter plot\n",
        "  ax.plot(x, y_hat, color='r', label='Fit')  # our estimated model\n",
        "  # plot residuals\n",
        "  ymin = np.minimum(y, y_hat)\n",
        "  ymax = np.maximum(y, y_hat)\n",
        "  ax.vlines(x, ymin, ymax, 'g', alpha=0.5, label='Residuals')\n",
        "  ax.set(\n",
        "      title=fr\"$\\hat{{\\theta}}$ = {theta_hat:0.2f}, MSE = {mse(x, y, theta_hat):.2f}\",\n",
        "      xlabel='x',\n",
        "      ylabel='y'\n",
        "  )\n",
        "  ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8oT_LhRsjUO"
      },
      "source": [
        "---\n",
        "# Sección 1: Error cuadrático medio (MSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDYAKUpOsjUT"
      },
      "source": [
        "**La regresión lineal de mínimos cuadrados** es un procedimiento de optimización antiguo pero de oro que vamos a utilizar para el ajuste de datos. Los problemas de optimización de mínimos cuadrados (LS) son aquellos en los que la función objetivo es una función cuadrática de los parámetros que se optimizan.\n",
        "\n",
        "Suponga que tiene un conjunto de medidas, $ y_ {n} $ (la variable \"dependiente\") obtenida para diferentes valores de entrada, $ x_ {n} $ (la variable \"independiente\" o \"explicativa\"). Supongamos que creemos que las medidas son proporcionales a los valores de entrada, pero están dañadas por algunos errores de medida (aleatorios), $ \\epsilon_ {n} $, es decir:\n",
        "\n",
        "$$y_{n}= \\theta x_{n}+\\epsilon_{n}$$\n",
        "\n",
        "para algún parámetro de pendiente desconocido $ \\ theta. $ El problema de regresión de mínimos cuadrados utiliza ** error cuadrático medio (MSE) ** como su función objetivo, su objetivo es encontrar el valor del parámetro $ \\theta $ minimizando el promedio de cuadrados errores:\n",
        "\n",
        "\\begin{align}\n",
        "\\min _{\\theta} \\frac{1}{N}\\sum_{n=1}^{N}\\left(y_{n}-\\theta x_{n}\\right)^{2}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd2KW0UZsjUT"
      },
      "source": [
        "Ahora exploraremos cómo se usa MSE para ajustar un modelo de regresión lineal a los datos. Con fines ilustrativos, crearemos un conjunto de datos sintéticos simple donde conocemos el verdadero modelo subyacente. Esto nos permitirá ver cómo se comparan nuestros esfuerzos de estimación para descubrir el modelo real (aunque en la práctica rara vez tenemos este lujo).\n",
        "\n",
        "Primero, generaremos algunas muestras ruidosas $ x $ de [0, 10) a lo largo de la línea $ y = 1.2x $ como nuestro conjunto de datos al que deseamos ajustar un modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0qWU8HWsjUU"
      },
      "source": [
        "# @title\n",
        "\n",
        "# @markdown Ejecute esta celda para generar algunos datos simulados\n",
        "\n",
        "# setting a fixed seed to our random number generator ensures we will always\n",
        "# get the same psuedorandom number sequence\n",
        "np.random.seed(121)\n",
        "\n",
        "# Let's set some parameters\n",
        "theta = 1.2\n",
        "n_samples = 30\n",
        "\n",
        "# Draw x and then calculate y\n",
        "x = 10 * np.random.rand(n_samples)  # sample from a uniform distribution over [0,10)\n",
        "noise = np.random.randn(n_samples)  # sample from a standard normal distribution\n",
        "y = theta * x + noise\n",
        "\n",
        "# Plot the results\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(x, y)  # produces a scatter plot\n",
        "ax.set(xlabel='x', ylabel='y');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSuHX01YsjUZ"
      },
      "source": [
        "Ahora que tenemos nuestro conjunto de datos adecuadamente ruidoso, podemos comenzar a intentar estimar el modelo subyacente que lo produjo. Usamos MSE para evaluar qué tan exitosa es una estimación de pendiente particular $ \\hat {\\theta} $ para explicar los datos, cuanto más cerca de 0 esté el MSE, mejor se ajusta nuestra estimación a los datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s63U9cmUsjUZ"
      },
      "source": [
        "## Ejercicio 1: Computar el MSE\n",
        "\n",
        "En este ejercicio, implementará un método para calcular el error cuadrático medio para un conjunto de entradas $ x $, medidas $ y $ y estimación de pendiente $ \\hat{\\theta} $. Luego calcularemos e imprimiremos el error cuadrático medio para 3 opciones diferentes de theta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paj-aJBWsjUa"
      },
      "source": [
        "def mse(x, y, theta_hat):\n",
        "  \"\"\"Compute the mean squared error\n",
        "\n",
        "  Args:\n",
        "    x (ndarray): Un array de tamaño (samples,) que contiene los valores de entrada.\n",
        "    y (ndarray): Un array de tamaño (samples,) que contiene los valores de medición correspondientes a las entradas.\n",
        "    theta_hat (float): Un estimado del parámetro de pendiente\n",
        "\n",
        "  Returns:\n",
        "    float: El error cuadrático medio de los datos con el parámetro estimado.\n",
        "  \"\"\"\n",
        "\n",
        "  # Computar el y estimado\n",
        "  y_hat = ...\n",
        "\n",
        "  # Computar el error cuadrático medio\n",
        "  mse = ...\n",
        "\n",
        "  return mse\n",
        "\n",
        "\n",
        "# Descomente a continuación para probar su función\n",
        "theta_hats = [0.75, 1.0, 1.5]\n",
        "# for theta_hat in theta_hats:\n",
        "#   print(f\"theta_hat de {theta_hat} tiene un MSE de {mse(x, y, theta_hat):.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellView": "both",
        "outputId": "3ed61eef-3d5e-4118-b0b8-dc2a299c461a",
        "id": "4QZg-Xt5sjUe"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D3_ModelFitting/solutions/W1D3_Tutorial1_Solution_12a57de0.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHcPUaQ-sjUf"
      },
      "source": [
        "El resultado debe ser:\n",
        "\n",
        "theta_hat of 0.75 has an MSE of 9.08\\\n",
        "theta_hat of 1.0 has an MSE of 3.0\\\n",
        "theta_hat of 1.5 has an MSE of 4.52\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms5VOwxqsjUg"
      },
      "source": [
        "Vemos que $ \\hat{\\theta} = 1.0 $ es nuestra mejor estimación de las tres que probamos. Sin embargo, mirar solo los números sin procesar no siempre es satisfactorio, así que visualicemos cómo se ve nuestro modelo estimado sobre los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nY1_N0psjUg"
      },
      "source": [
        "#@title\n",
        "\n",
        "#@markdown Ejecute esta celda para visualizar modelos estimados\n",
        "\n",
        "fig, axes = plt.subplots(ncols=3, figsize=(18, 4))\n",
        "for theta_hat, ax in zip(theta_hats, axes):\n",
        "\n",
        "  # True data\n",
        "  ax.scatter(x, y, label='Observed')  # our data scatter plot\n",
        "\n",
        "  # Compute and plot predictions\n",
        "  y_hat = theta_hat * x\n",
        "  ax.plot(x, y_hat, color='r', label='Fit')  # our estimated model\n",
        "\n",
        "  ax.set(\n",
        "      title= fr'$\\hat{{\\theta}}$= {theta_hat}, MSE = {mse(x, y, theta_hat):.2f}',\n",
        "      xlabel='x',\n",
        "      ylabel='y'\n",
        "  );\n",
        "\n",
        "axes[0].legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDGwsBXysjUk"
      },
      "source": [
        "## Demo Interactivo: Explorador de MSE\n",
        "\n",
        "Usando un widget interactivo, podemos ver fácilmente cómo cambiar nuestra estimación de pendiente cambia el ajuste de nuestro modelo. Mostramos los **residuales**, las diferencias entre los datos observados y los predichos, como segmentos de línea entre el punto de datos (respuesta observada) y la respuesta predicha correspondiente en la línea de ajuste del modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlG0fzsUsjUl",
        "outputId": "8d0a3518-51d3-4b30-a29c-2d2bade53f92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "#@title\n",
        "\n",
        "#@markdown ¡Asegúrese de ejecutar esta celda para habilitar el widget!\n",
        "\n",
        "@widgets.interact(theta_hat=widgets.FloatSlider(1.0, min=0.0, max=2.0))\n",
        "def plot_data_estimate(theta_hat):\n",
        "  y_hat = theta_hat * x\n",
        "  plot_observed_vs_predicted(x, y, y_hat, theta_hat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-52de5b9a0199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#@markdown <- ¡Asegúrese de ejecutar esta celda para habilitar el widget!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mwidgets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_hat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidgets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatSlider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_data_estimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta_hat\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'widgets' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jly2fY6qsjUo"
      },
      "source": [
        "Si bien explorar visualmente varias estimaciones puede ser instructivo, no es la más eficiente para encontrar la mejor estimación que se ajuste a nuestros datos. Otra técnica que podemos usar es elegir un rango razonable de valores de parámetros y calcular el MSE en varios valores en ese intervalo. Esto nos permite graficar el error contra el valor del parámetro (esto también se llama **panorama de errores**, especialmente cuando tratamos con más de un parámetro). Podemos seleccionar el $ \\hat{\\theta} $ ($ \\hat{\\theta}_{MSE} $) final como el que resulte en el error más bajo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQB8HBwgsjUp"
      },
      "source": [
        "# @title\n",
        "\n",
        "# @markdown Ejecute esta celda para recorrer theta_hats, calcular MSE y trazar resultados\n",
        "\n",
        "# Loop over different thetas, compute MSE for each\n",
        "theta_hat_grid = np.linspace(-2.0, 4.0)\n",
        "errors = np.zeros(len(theta_hat_grid))\n",
        "for i, theta_hat in enumerate(theta_hat_grid):\n",
        "  errors[i] = mse(x, y, theta_hat)\n",
        "\n",
        "# Find theta that results in lowest error\n",
        "best_error = np.min(errors)\n",
        "theta_hat = theta_hat_grid[np.argmin(errors)]\n",
        "\n",
        "\n",
        "# Plot results\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(theta_hat_grid, errors, '-o', label='MSE', c='C1')\n",
        "ax.axvline(theta, color='g', ls='--', label=r\"$\\theta_{True}$\")\n",
        "ax.axvline(theta_hat, color='r', ls='-', label=r\"$\\hat{{\\theta}}_{MSE}$\")\n",
        "ax.set(\n",
        "  title=fr\"Best fit: $\\hat{{\\theta}}$ = {theta_hat:.2f}, MSE = {best_error:.2f}\",\n",
        "  xlabel=r\"$\\hat{{\\theta}}$\",\n",
        "  ylabel='MSE')\n",
        "ax.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmIwVayPsjUt"
      },
      "source": [
        "Podemos ver que nuestro mejor ajuste es $ \\hat{\\theta} = 1,18 $ con un MSE de 1,45. ¡Esto está bastante cerca del valor verdadero original $ \\ theta = 1.2 $!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zBni3DYsjUu"
      },
      "source": [
        "---\n",
        "# Sección 2: Optimización de mínimos cuadrados\n",
        "\n",
        "\n",
        "Si bien el enfoque detallado anteriormente (calcular MSE en varios valores de $ \\hat\\theta $) nos llevó rápidamente a una buena estimación, aún se basaba en evaluar el valor de MSE en una cuadrícula de valores especificados a mano. Si no elegimos un buen rango para empezar, o con suficiente granularidad, podríamos perder el mejor estimador posible. Vayamos un paso más allá, y en lugar de encontrar el MSE mínimo a partir de un conjunto de estimaciones candidatas, resolvamos analíticamente.\n",
        "\n",
        "Podemos hacer esto minimizando la función de costo. El error cuadrático medio es una función objetivo convexa, por lo tanto, podemos calcular su mínimo mediante el cálculo. Después de calcular el mínimo, encontramos que:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat\\theta = \\frac{\\vec{x}^\\top \\vec{y}}{\\vec{x}^\\top \\vec{x}}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lWcfmcCsjUu"
      },
      "source": [
        "### Ejercicio 2: Resuelva para el estimador óptimo\n",
        "\n",
        "En este ejercicio, escribirás una función que encuentre el valor óptimo de $ \\hat{\\theta} $ usando el enfoque de optimización de mínimos cuadrados (la ecuación anterior) para resolver la minimización de MSE. Debe tomar los argumentos $ x $ y $ y $ y devolver la solución $ \\hat{\\theta} $.\n",
        "\n",
        "Luego usaremos su función para calcular $ \\hat {\\theta} $ y graficar la predicción resultante sobre los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecgfCliTsjUv"
      },
      "source": [
        "def solve_normal_eqn(x, y):\n",
        "  \"\"\"Resolver las ecuaciones normales para producir el valor de theta_hat que minimiza MSE.\n",
        "\n",
        "    Args:\n",
        "    x (ndarray): Un array de tamaño (samples,) que contiene los valores de entrada.\n",
        "    y (ndarray): Un array de tamaño (samples,) que contiene los valores de medición correspondientes a las entradas.\n",
        "\n",
        "  Returns:\n",
        "    float: el valor de theta_hat arribado de minizar el MSE\n",
        "  \"\"\"\n",
        "\n",
        "  # Computar theta_hat analíticamente\n",
        "  theta_hat = ...\n",
        "\n",
        "  return theta_hat\n",
        "\n",
        "\n",
        "# Descomente a continuación para probar su función\n",
        "# theta_hat = solve_normal_eqn(x, y)\n",
        "# y_hat = theta_hat * x\n",
        "# plot_observed_vs_predicted(x, y, y_hat, theta_hat)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellView": "both",
        "outputId": "5a46f798-31e1-4fbd-da7a-de8a270c5f7d",
        "id": "TFs7PIjFsjUz"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D3_ModelFitting/solutions/W1D3_Tutorial1_Solution_7a89ba24.py)\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=558 height=414 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D3_ModelFitting/static/W1D3_Tutorial1_Solution_7a89ba24_0.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ2u3FwesjU0"
      },
      "source": [
        "Vemos que la solución analítica produce un resultado incluso mejor que nuestra búsqueda de cuadrícula anterior, ¡produciendo $ \\hat{\\theta} = 1.21 $ con MSE = 1.43!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzSCfnj8sjU0"
      },
      "source": [
        "---\n",
        "# Resumen\n",
        "\n",
        "- La regresión lineal de mínimos cuadrados es un procedimiento de optimización que se puede utilizar para el ajuste de datos:\n",
        "     - Tarea: predice un valor para $ y $ dado $ x $\n",
        "     - Medida de rendimiento: $ \\textrm {MSE} $\n",
        "     - Procedimiento: minimizar $ \\textrm {MSE} $ resolviendo las ecuaciones normales\n",
        "- **Punto clave**: ajustamos el modelo definiendo una *función objetivo* y minimizándola.\n",
        "- **Nota**: En este caso, existe una solución *analítica* para el problema de minimización y, en la práctica, esta solución se puede calcular usando *álgebra lineal*. Esto es *extremadamente* poderoso y forma la base de gran parte del cálculo numérico en todas las ciencias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDXznBk7sjU1"
      },
      "source": [
        "---\n",
        "# Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP1ltA9RsjU2"
      },
      "source": [
        "## Derivación de optimización de mínimos cuadrados\n",
        "\n",
        "Aquí describiremos la derivación de la solución de mínimos cuadrados.\n",
        "\n",
        "Primero establecemos la derivada de la expresión de error con respecto a $ \\theta $ igual a cero,\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{d}{d\\theta}\\frac{1}{N}\\sum_{i=1}^N(y_i - \\theta x_i)^2 = 0 \\\\\n",
        "\\frac{1}{N}\\sum_{i=1}^N-2x_i(y_i - \\theta x_i) = 0\n",
        "\\end{align}\n",
        "\n",
        "donde usamos la regla de la cadena. Ahora, despejando $ \\theta $, obtenemos un valor óptimo de:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat\\theta = \\frac{\\sum_{i=1}^N x_i y_i}{\\sum_{i=1}^N x_i^2}\n",
        "\\end{align}\n",
        "\n",
        "Que podemos escribir en notación vectorial como:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat\\theta = \\frac{\\vec{x}^\\top \\vec{y}}{\\vec{x}^\\top \\vec{x}}\n",
        "\\end{align}"
      ]
    }
  ]
}