{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "name": "Tutorial4",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/german1728/Chimera/blob/master/Tutorial4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcxYzk_kfIpQ"
      },
      "source": [
        "# Tutorial 4\n",
        "# Ajuste del modelo: regresión lineal múltiple y regresión polinomial\n",
        "\n",
        "**Content creators**: Pierre-Étienne Fiquet, Anqi Wu, Alex Hyafil with help from Byron Galbraith, Ella Batty\n",
        "\n",
        "**Content reviewers**: Lina Teichmann, Saeed Salehi, Patrick Mineault, Michael Waskom\n",
        "\n",
        "**Content traduction**: Israel Chaparro\n",
        "\n",
        "Source: Neuromatch Academy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAT3fhYRfIpS"
      },
      "source": [
        "---\n",
        "#Objetivos del Tutorial\n",
        "\n",
        "Este es el Tutorial 4 de una serie sobre cómo ajustar modelos a datos. Comenzamos con regresión lineal simple, usando optimización de mínimos cuadrados (Tutorial 1) y Estimación de máxima verosimilitud (Tutorial 2). Usaremos bootstrapping para construir intervalos de confianza alrededor de los parámetros del modelo lineal inferido (Tutorial 3). Terminaremos nuestra exploración de modelos de regresión generalizando a la regresión lineal múltiple y la regresión polinomial (Tutorial 4). Terminamos aprendiendo a elegir entre estos distintos modelos. Analizamos la compensación de sesgo-varianza (Tutorial 5) y la Validación cruzada para la selección del modelo (Tutorial 6).\n",
        "\n",
        "En este tutorial, generalizaremos el modelo de regresión para incorporar múltiples características.\n",
        "- Aprenda a estructurar las entradas para la regresión utilizando la 'Matriz de diseño'\n",
        "- Generalizar el MSE para múltiples características usando el estimador de mínimos cuadrados ordinario\n",
        "- Visualice datos y ajuste del modelo en múltiples dimensiones\n",
        "- Ajustar modelos de regresión polinomial de diferente complejidad\n",
        "- Trazar y evaluar los ajustes de regresión polinomial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xmv5482fIpY"
      },
      "source": [
        "---\n",
        "# Configuración"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "3v-r-9QgfIpZ"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X2FZ0RvfIpg"
      },
      "source": [
        "#@title Configuración de Figuras\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uju5REafIpl"
      },
      "source": [
        "#@title Funciones de Ayuda\n",
        "\n",
        "def plot_fitted_polynomials(x, y, theta_hat):\n",
        "  \"\"\" Plot polynomials of different orders\n",
        "\n",
        "  Args:\n",
        "    x (ndarray): input vector of shape (n_samples)\n",
        "    y (ndarray): vector of measurements of shape (n_samples)\n",
        "    theta_hat (dict): polynomial regression weights for different orders\n",
        "  \"\"\"\n",
        "\n",
        "  x_grid = np.linspace(x.min() - .5, x.max() + .5)\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  for order in range(0, max_order + 1):\n",
        "    X_design = make_design_matrix(x_grid, order)\n",
        "    plt.plot(x_grid, X_design @ theta_hat[order]);\n",
        "\n",
        "  plt.ylabel('y')\n",
        "  plt.xlabel('x')\n",
        "  plt.plot(x, y, 'C0.');\n",
        "  plt.legend([f'order {o}' for o in range(max_order + 1)], loc=1)\n",
        "  plt.title('polynomial fits')\n",
        "  plt.show()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNW4pQd9fIpp"
      },
      "source": [
        "---\n",
        "# Sección 1: Regresión lineal múltiple\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHMZPtRnfIpq"
      },
      "source": [
        "Ahora que hemos considerado el caso univariado y cómo producir intervalos de confianza para nuestro estimador, pasamos al caso de regresión lineal general, donde podemos tener más de un regresor o característica en nuestra entrada.\n",
        "\n",
        "Recuerde que nuestro modelo lineal univariado original se dio como\n",
        "\n",
        "\\begin{align}\n",
        "y = \\theta x + \\epsilon\n",
        "\\end{align}\n",
        "\n",
        "donde $ \\theta $ es la pendiente y $ \\epsilon $ algo de ruido. Podemos extender esto fácilmente al escenario multivariado agregando otro parámetro para cada característica adicional\n",
        "\n",
        "\\begin{align}\n",
        "y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... +\\theta_d x_d + \\epsilon\n",
        "\\end{align}\n",
        "\n",
        "donde $ \\theta_0 $ es la intersección y $ d $ es el número de características (también es la dimensionalidad de nuestra entrada).\n",
        "\n",
        "Podemos condensar esto sucintamente usando notación vectorial para un solo punto de datos\n",
        "\n",
        "\\begin{align}\n",
        "y_i = \\boldsymbol{\\theta}^{\\top}\\mathbf{x}_i + \\epsilon\n",
        "\\end{align}\n",
        "\n",
        "y completamente en forma de matriz\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\theta} + \\mathbf{\\epsilon}\n",
        "\\end{align}\n",
        "\n",
        "donde $ \\mathbf {y} $ es un vector de medidas, $ \\mathbf {X} $ es una matriz que contiene los valores de las características (columnas) para cada muestra de entrada (filas), y $ \\boldsymbol{\\theta} $ es nuestro vector de parámetro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ybwVqeZfIpr"
      },
      "source": [
        "Para este tutorial nos centraremos en el caso bidimensional ($ d = 2 $), que nos permite visualizar fácilmente nuestros resultados. Como ejemplo, piense en una situación en la que un científico registra la respuesta en picos de una célula ganglionar de la retina a patrones de señales de luz que varían en contraste y orientación. Luego, los valores de contraste y orientación se pueden usar como características / regresores para predecir la respuesta de las células.\n",
        "\n",
        "En este caso, nuestro modelo se puede escribir como:\n",
        "\n",
        "\\begin{align}\n",
        "y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\epsilon\n",
        "\\end{align}\n",
        "\n",
        "o en forma de matriz donde\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{X} = \n",
        "\\begin{bmatrix}\n",
        "1 & x_{1,1} & x_{1,2} \\\\\n",
        "1 & x_{2,1} & x_{2,2} \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "1 & x_{n,1} & x_{n,2}\n",
        "\\end{bmatrix}, \n",
        "\\boldsymbol{\\theta} =\n",
        "\\begin{bmatrix}\n",
        "\\theta_0 \\\\\n",
        "\\theta_1 \\\\\n",
        "\\theta_2 \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Para nuestro conjunto de datos de exploración real, estableceremos $ \\boldsymbol{\\theta} = [0, -2, -3] $ y sacaremos $ N = 40 $ muestras ruidosas de $ x \\in [-2,2) $. Tenga en cuenta que establecer el valor de $ \\theta_0 = 0 $ efectivamente ignora el término de compensación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMlzzQu_fIpr"
      },
      "source": [
        "#@title\n",
        "#@markdown Ejecute esta celda para simular algunos datos\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(1234)\n",
        "\n",
        "# Set parameters\n",
        "theta = [0, -2, -3]\n",
        "n_samples = 40\n",
        "\n",
        "# Draw x and calculate y\n",
        "n_regressors = len(theta)\n",
        "x0 = np.ones((n_samples, 1))\n",
        "x1 = np.random.uniform(-2, 2, (n_samples, 1))\n",
        "x2 = np.random.uniform(-2, 2, (n_samples, 1))\n",
        "X = np.hstack((x0, x1, x2))\n",
        "noise = np.random.randn(n_samples)\n",
        "y = X @ theta + noise\n",
        "\n",
        "\n",
        "ax = plt.subplot(projection='3d')\n",
        "ax.plot(X[:,1], X[:,2], y, '.')\n",
        "\n",
        "ax.set(\n",
        "    xlabel='$x_1$',\n",
        "    ylabel='$x_2$',\n",
        "    zlabel='y'\n",
        ")\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pGrqob3fIpx"
      },
      "source": [
        "Ahora que tenemos nuestro conjunto de datos, queremos encontrar un vector óptimo de parámetros $ \\boldsymbol {\\hat\\theta} $. Recuerde nuestra solución analítica para minimizar MSE para un solo regresor:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat\\theta = \\frac{\\sum_{i=1}^N x_i y_i}{\\sum_{i=1}^N x_i^2}.\n",
        "\\end{align}\n",
        "\n",
        "Lo mismo es cierto para el caso del regresor múltiple, solo que ahora se expresa en forma de matriz\n",
        "\n",
        "\\begin{align}\n",
        "\\boldsymbol{\\hat\\theta} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}.\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm3TnoCHfIpy"
      },
      "source": [
        "### Ejercicio 1: Estimador ordinario de mínimos cuadrados\n",
        "\n",
        "En este ejercicio, implementará el enfoque MCO para estimar $ \\boldsymbol{\\hat \\theta} $ a partir de la matriz de diseño $ \\mathbf {X} $ y el vector de medición $ \\mathbf {y} $. Puede utilizar el símbolo `@` para la multiplicación de matrices, `.T` para la transposición y` np.linalg.inv` para la inversión de matrices.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4v9Om7_fIpz"
      },
      "source": [
        "def ordinary_least_squares(X, y):\n",
        "  \"\"\"Ordinary least squares estimator for linear regression.\n",
        "\n",
        "  Args:\n",
        "    x (ndarray): design matrix of shape (n_samples, n_regressors)\n",
        "    y (ndarray): vector of measurements of shape (n_samples)\n",
        "\n",
        "  Returns:\n",
        "    ndarray: estimated parameter values of shape (n_regressors)\n",
        "  \"\"\"\n",
        "\n",
        "  # Computar theta_hat usando OLS\n",
        "  theta_hat = ...\n",
        "\n",
        "  return theta_hat\n",
        "\n",
        "\n",
        "# Descomente a continuación para probar su función\n",
        "# theta_hat = ordinary_least_squares(X, y)\n",
        "# print(theta_hat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellView": "both",
        "outputId": "8132759a-2b3d-4840-a38e-11390cc50118",
        "id": "w0Xn7al8fIp3"
      },
      "source": [
        "[*Haga clic para obtener una solución*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D3_ModelFitting/solutions/W1D3_Tutorial4_Solution_2b5abc38.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyvZAXDFfIp4"
      },
      "source": [
        "Después de completar esta función, debería ver que $\\hat{\\theta}$ = [ 0.13861386, -2.09395731, -3.16370742]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC_gLNsNfIp4"
      },
      "source": [
        "Ahora que tenemos nuestro $ \\mathbf {\\hat \\theta} $, podemos obtener $ \\mathbf {\\hat y} $ y por lo tanto nuestro error cuadrático medio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pb6QSgXGfIp5"
      },
      "source": [
        "# Calcular datos predichos\n",
        "theta_hat = ordinary_least_squares(X, y)\n",
        "y_hat = X @ theta_hat\n",
        "\n",
        "# Computar MSE\n",
        "print(f\"MSE = {np.mean((y - y_hat)**2):.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ3JT4p5fIp9"
      },
      "source": [
        "Finalmente, el siguiente código trazará una visualización geométrica de los puntos de datos (azul) y el plano ajustado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQsujuBQfIp-"
      },
      "source": [
        "#@title\n",
        "#@markdown Ejecute esta celda para visualizar datos y plano predicho\n",
        "\n",
        "theta_hat = ordinary_least_squares(X, y)\n",
        "xx, yy = np.mgrid[-2:2:50j, -2:2:50j]\n",
        "y_hat_grid = np.array([xx.flatten(), yy.flatten()]).T @ theta_hat[1:]\n",
        "y_hat_grid = y_hat_grid.reshape((50, 50))\n",
        "\n",
        "ax = plt.subplot(projection='3d')\n",
        "ax.plot(X[:, 1], X[:, 2], y, '.')\n",
        "ax.plot_surface(xx, yy, y_hat_grid, linewidth=0, alpha=0.5, color='C1',\n",
        "                cmap=plt.get_cmap('coolwarm'))\n",
        "\n",
        "for i in range(len(X)):\n",
        "  ax.plot((X[i, 1], X[i, 1]),\n",
        "          (X[i, 2], X[i, 2]),\n",
        "          (y[i], y_hat[i]),\n",
        "          'g-', alpha=.5)\n",
        "\n",
        "ax.set(\n",
        "    xlabel='$x_1$',\n",
        "    ylabel='$x_2$',\n",
        "    zlabel='y'\n",
        ")\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG4fZZaRfIqC"
      },
      "source": [
        "---\n",
        "# Sección 2: Regresión polinomial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoCOLUR0fIqD"
      },
      "source": [
        "Hasta ahora, aprendió a predecir resultados a partir de entradas ajustando un modelo de regresión lineal. Ahora podemos modelar todo tipo de relaciones, ¡incluso en neurociencia!\n",
        "\n",
        "Un problema potencial con este enfoque es la simplicidad del modelo. La regresión lineal, como su nombre lo indica, solo puede capturar una relación lineal entre las entradas y las salidas. Dicho de otra manera, las salidas previstas son solo una suma ponderada de las entradas. ¿Qué pasa si ocurren cálculos más complicados? Afortunadamente, existen muchos modelos más complejos (y encontrará muchos más en las próximas 3 semanas). Un modelo que aún es muy simple de ajustar y comprender, pero que captura relaciones más complejas, es la **regresión polinomial**, una extensión de la regresión lineal.\n",
        "\n",
        "Dado que la regresión polinomial es una extensión de la regresión lineal, ¡todo lo que aprendiste hasta ahora te resultará útil! El objetivo es el mismo: queremos predecir la variable dependiente $ y_ {n} $ dados los valores de entrada $ x_ {n} $. El cambio clave es el tipo de relación entre entradas y salidas que el modelo puede capturar.\n",
        "\n",
        "Los modelos de regresión lineal predicen las salidas como una suma ponderada de las entradas:\n",
        "\n",
        "$$y_{n}= \\theta_0 + \\theta x_{n} + \\epsilon_{n}$$\n",
        "\n",
        "Con la regresión polinomial, modelamos las salidas como una ecuación polinomial basada en las entradas. Por ejemplo, podemos modelar las salidas como:\n",
        "\n",
        "$$y_{n}= \\theta_0 + \\theta_1 x_{n} + \\theta_2 x_{n}^2 + \\theta_3 x_{n}^3 + \\epsilon_{n}$$\n",
        "\n",
        "Podemos cambiar la complejidad de un polinomio cambiando el orden del polinomio. El orden de un polinomio se refiere a la potencia más alta del polinomio. La ecuación anterior es un polinomio de tercer orden porque el valor más alto de x se eleva a 3. Podríamos agregar otro término ($ + \\theta_4 x_{n} ^ 4 $) para modelar un polinomio de orden 4 y así sucesivamente.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYqInuwYfIqE"
      },
      "source": [
        "Primero, simularemos algunos datos para practicar el ajuste de modelos de regresión polinomial. Generaremos entradas aleatorias $ x $ y luego calcularemos y de acuerdo con $ y = x ^ 2 - x - 2 $, con algo de ruido adicional tanto en la entrada como en la salida para hacer que el ejercicio de ajuste del modelo se acerque más a una situación de la vida real."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3ygXengfIqF"
      },
      "source": [
        "#@title\n",
        "#@markdown Ejecute esta celda para simular algunos datos\n",
        "\n",
        "# setting a fixed seed to our random number generator ensures we will always\n",
        "# get the same psuedorandom number sequence\n",
        "np.random.seed(121)\n",
        "n_samples = 30\n",
        "x = np.random.uniform(-2, 2.5, n_samples)  # inputs uniformly sampled from [-2, 2.5)\n",
        "y =  x**2 - x - 2   # computing the outputs\n",
        "\n",
        "output_noise = 1/8 * np.random.randn(n_samples)\n",
        "y += output_noise  # adding some output noise\n",
        "\n",
        "input_noise = 1/2 * np.random.randn(n_samples)\n",
        "x += input_noise  # adding some input noise\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(x, y)  # produces a scatter plot\n",
        "ax.set(xlabel='x', ylabel='y');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kexeb3hkfIqJ"
      },
      "source": [
        "## Sección 2.1: Matriz de diseño para regresión polinomial\n",
        "\n",
        "Ahora que tenemos la idea básica de la regresión polinomial y algunos datos ruidosos, ¡comencemos! La diferencia clave entre ajustar un modelo de regresión lineal y un modelo de regresión polinomial radica en cómo estructuramos las variables de entrada.\n",
        "\n",
        "Para la regresión lineal, usamos $ X = x $ como datos de entrada. Para agregar un sesgo constante (una intersección con el eje y en una gráfica 2-D), usamos $ X = \\big[\\boldsymbol 1, x \\big] $, donde $ \\boldsymbol 1 $ es una columna de unos. Al ajustar, aprendemos un peso para cada columna de esta matriz. Entonces aprendemos un peso que se multiplica con la columna 1; en este caso, esa columna es todos unos, por lo que obtenemos el parámetro de sesgo ($ + \\theta_0 $). También aprendemos un peso para cada columna, o cada característica de x, como aprendimos en la Sección 1.\n",
        "\n",
        "Esta matriz $ X $ que usamos para nuestras entradas se conoce como **matriz de diseño**. Queremos crear nuestra matriz de diseño para que aprendamos los pesos de $ x ^ 2, x ^ 3, $, etc. Por lo tanto, queremos construir nuestra matriz de diseño $ X $ para la regresión polinomial de orden $ k $ como:\n",
        "\n",
        "$$ X = \\big [\\boldsymbol 1, x ^ 1, x ^ 2, \\ldots, x ^ k \\big], $$\n",
        "\n",
        "donde $ \\boldsymbol {1} ​​$ es el vector de la misma longitud que $ x $ que consta de todos unos, y $ x ^ p $ es el vector o matriz $ x $ con todos los elementos elevados a la potencia $ p $. Tenga en cuenta que $ \\boldsymbol {1} ​​= x ^ 0 $ y $ x ^ 1 = x $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eYLer7nfIqK"
      },
      "source": [
        "### Ejercicio 2: Matriz de diseño de estructura\n",
        "\n",
        "Cree una función (`make_design_matrix`) que estructura la matriz de diseño dados los datos de entrada y el orden del polinomio que desea ajustar. Imprimiremos parte de esta matriz de diseño para nuestros datos y ordenaremos 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhagR-2YfIqL"
      },
      "source": [
        "def make_design_matrix(x, order):\n",
        "  \"\"\"Create the design matrix of inputs for use in polynomial regression\n",
        "\n",
        "  Args:\n",
        "    x (ndarray): input vector of shape (n_samples)\n",
        "    order (scalar): polynomial regression order\n",
        "\n",
        "  Returns:\n",
        "    ndarray: design matrix for polynomial regression of shape (samples, order+1)\n",
        "  \"\"\"\n",
        "  # Transformamos a la dimensión (n x 1) para que funcione\n",
        "  if x.ndim == 1:\n",
        "    x = x[:, None]\n",
        "\n",
        "  #si x tiene más de una característica, no queremos múltiples columnas de unos, así que asignamos x^0 aquí\n",
        "  design_matrix = np.ones((x.shape[0], 1))\n",
        "\n",
        "  # Recorre el resto de grados y apila columnas (pista: np.hstack)\n",
        "  for degree in range(1, order + 1):\n",
        "      design_matrix = ...\n",
        "\n",
        "  return design_matrix\n",
        "\n",
        "\n",
        "# Descomenta para probar tu función\n",
        "order = 5\n",
        "# X_design = make_design_matrix(x, order)\n",
        "# print(X_design[0:2, 0:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellView": "both",
        "outputId": "f39aeb8b-d933-49f1-ad9f-3fbbab781871",
        "id": "LTY7_GPjfIqP"
      },
      "source": [
        "[*Haga clic para obtener una solución*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D3_ModelFitting/solutions/W1D3_Tutorial4_Solution_f8576b48.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXDhE9yTfIqP"
      },
      "source": [
        "Debería ver que la sección impresa de esta matriz de diseño es `[[ 1.         -1.51194917]\n",
        " [ 1.         -0.35259945]]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBzinoNLfIqQ"
      },
      "source": [
        "## Sección 2.2: Ajuste de modelos de regresión polinomial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEnrMnsNfIqR"
      },
      "source": [
        "Ahora que tenemos las entradas estructuradas correctamente en nuestra matriz de diseño, ajustar una regresión polinomial es lo mismo que ajustar un modelo de regresión lineal. Toda la estructura polinomial que necesitamos aprender está contenida en cómo se estructuran las entradas en la matriz de diseño. Podemos usar la misma solución de mínimos cuadrados que calculamos en ejercicios anteriores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX1OrFNvfIqS"
      },
      "source": [
        "### Ejercicio 3: Ajuste de modelos de regresión polinomial con diferentes órdenes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q_ViIetfIqT"
      },
      "source": [
        "Aquí, ajustaremos modelos de regresión polinomial para encontrar los coeficientes de regresión ($ \\theta_0, \\theta_1, \\theta_2, $ ...) resolviendo el problema de mínimos cuadrados. Cree una función `solve_poly_reg` que recorra polinomios de diferentes órdenes (hasta ` max_order`), se ajuste a ese modelo y guarde los pesos para cada uno. Puede invocar la función `common_least_squares`.\n",
        "\n",
        "Luego, inspeccionaremos cualitativamente la calidad de nuestros ajustes para cada orden trazando los polinomios ajustados sobre los datos. Para ver curvas suaves, evaluamos los polinomios ajustados en una cuadrícula de valores $ x $ (que varían entre la mayor y la menor de las entradas presentes en el conjunto de datos)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEf21J87fIqT"
      },
      "source": [
        "def solve_poly_reg(x, y, max_order):\n",
        "  \"\"\"Fit a polynomial regression model for each order 0 through max_order.\n",
        "\n",
        "  Args:\n",
        "    x (ndarray): input vector of shape (n_samples)\n",
        "    y (ndarray): vector of measurements of shape (n_samples)\n",
        "    max_order (scalar): max order for polynomial fits\n",
        "\n",
        "  Returns:\n",
        "    dict: fitted weights for each polynomial model (dict key is order)\n",
        "  \"\"\"\n",
        "\n",
        "  # Create a dictionary with polynomial order as keys,\n",
        "  # and np array of theta_hat (weights) as the values\n",
        "  theta_hats = {}\n",
        "\n",
        "  # Loop over polynomial orders from 0 through max_order\n",
        "  for order in range(max_order + 1):\n",
        "\n",
        "    # Crear la matriz de diseño\n",
        "    X_design = ...\n",
        "\n",
        "    # Ajustar modelo polinomial\n",
        "    this_theta = ...\n",
        "\n",
        "    theta_hats[order] = this_theta\n",
        "\n",
        "  return theta_hats\n",
        "\n",
        "\n",
        "# Descomenta para probar tu función\n",
        "max_order = 5\n",
        "# theta_hats = solve_poly_reg(x, y, max_order)\n",
        "# plot_fitted_polynomials(x, y, theta_hats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellView": "both",
        "outputId": "426ccb6c-995c-4bc7-806c-6c3eb91c4bdd",
        "id": "1hSp6mNPfIqX"
      },
      "source": [
        "[*Haga clic para obtener una solución*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D3_ModelFitting/solutions/W1D3_Tutorial4_Solution_9b1eebd9.py)\n",
        "\n",
        "*Salida de ejemplo:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=560 height=416 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D3_ModelFitting/static/W1D3_Tutorial4_Solution_9b1eebd9_0.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thm04T6vfIqY"
      },
      "source": [
        "## Sección 2.4: Evaluación de la calidad del ajuste\n",
        "\n",
        "Al igual que con la regresión lineal, podemos calcular el error cuadrático medio (MSE) para tener una idea de qué tan bien se ajusta el modelo a los datos.\n",
        "\n",
        "Calculamos MSE como:\n",
        "\n",
        "$$ MSE = \\frac 1 N ||y - \\hat y||^2 = \\frac 1 N \\sum_{i=1}^N (y_i - \\hat y_i)^2 $$\n",
        "\n",
        "donde los valores predichos para cada modelo vienen dados por $ \\hat y = X \\hat \\theta $.\n",
        "\n",
        "* ¿Qué modelo (es decir, qué orden de polinomio) cree que tendrá el mejor MSE? *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oqV1zCUfIqZ"
      },
      "source": [
        "### Ejercicio 4: Calcule MSE y compare modelos\n",
        "\n",
        "Compararemos el MSE para diferentes órdenes de polinomios con un diagrama de barras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbGD16cafIqZ"
      },
      "source": [
        "mse_list = []\n",
        "order_list = list(range(max_order + 1))\n",
        "\n",
        "for order in order_list:\n",
        "\n",
        "  X_design = make_design_matrix(x, order)\n",
        "\n",
        "  # Obtenga la predicción para el modelo de regresión polinomial de este orden\n",
        "  y_hat = ...\n",
        "\n",
        "  # Calcule los residuos\n",
        "  residuals = ...\n",
        "\n",
        "  # Calcule el MSE\n",
        "  mse = ...\n",
        "\n",
        "  mse_list.append(mse)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Descomentar una vez que el ejercicio anterior esté completo\n",
        "# ax.bar(order_list, mse_list)\n",
        "\n",
        "ax.set(title='Comparing Polynomial Fits', xlabel='Polynomial order', ylabel='MSE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellView": "both",
        "outputId": "3fffc612-098e-409f-daa5-5439e9d1b613",
        "id": "MqO_MdDVfIqe"
      },
      "source": [
        "[*Haga clic para obtener una solución*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D3_ModelFitting/solutions/W1D3_Tutorial4_Solution_ccea5dd7.py)\n",
        "\n",
        "*Salida de Ejemplo:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=558 height=414 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D3_ModelFitting/static/W1D3_Tutorial4_Solution_ccea5dd7_0.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AdQT9DZfIqf"
      },
      "source": [
        "---\n",
        "# Resumen\n",
        "\n",
        "* La regresión lineal se generaliza naturalmente a múltiples dimensiones.\n",
        "* El álgebra lineal nos brinda las herramientas matemáticas para razonar y resolver tales problemas más allá del caso bidimensional\n",
        "\n",
        "* Para cambiar de un modelo de regresión lineal a un modelo de regresión polinomial, solo tenemos que cambiar cómo se estructuran los datos de entrada\n",
        "\n",
        "* Podemos elegir la complejidad del modelo cambiando el orden del ajuste del modelo polinomial\n",
        "\n",
        "* Los modelos polinomiales de orden superior tienden a tener un MSE más bajo en los datos con los que se ajustan\n",
        "\n",
        "** Nota **: En la práctica, los problemas multidimensionales de mínimos cuadrados se pueden resolver de manera muy eficiente (gracias a rutinas numéricas como LAPACK).\n",
        "\n"
      ]
    }
  ]
}